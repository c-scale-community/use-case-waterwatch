{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5887482a-4398-495b-8342-0f1308c76ffa",
   "metadata": {},
   "source": [
    "# Job Manageent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa2457c-2bd7-4ce0-ac66-b22322073d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Connection to 'https://openeo.vito.be/openeo/1.0' with OidcBearerAuth>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openeo import connect, Connection\n",
    "from openeo.rest.job import BatchJob\n",
    "\n",
    "# Connect to backends\n",
    "vito_url: str = \"https://openeo.vito.be/openeo/1.0\"\n",
    "vito_creo_url: str = \"https://openeo.creo.vito.be\"\n",
    "vito_dev_url: str = \"https://openeo-dev.vito.be/openeo/1.0/\"\n",
    "ee_url: str = \"https://earthengine.openeo.org\"\n",
    "openeo_platform: str = \"openeo.cloud\"\n",
    "\n",
    "# con: Connection = connect(ee_url).authenticate_basic(username=\"group1\", password=\"test123\")\n",
    "con: Connection = connect(vito_url)\n",
    "con.authenticate_oidc(provider_id=\"egi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10513715-1fb6-4a38-b2db-40148469a938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-data-table')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-data-table>\n",
       "        <script type=\"application/json\">{\"columns\": {\"id\": {\"name\": \"ID\", \"primaryKey\": true}, \"title\": {\"name\": \"Title\"}, \"status\": {\"name\": \"Status\"}, \"created\": {\"name\": \"Submitted\", \"format\": \"Timestamp\", \"sort\": \"desc\"}, \"updated\": {\"name\": \"Last update\", \"format\": \"Timestamp\"}}, \"data\": [{\"created\": \"2022-08-02T12:46:33Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-0b80545f29df49a0b777ac196f3d2881\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-02T12:47:01Z\"}, {\"created\": \"2022-08-08T07:15:09Z\", \"description\": \"get timestamps for aggregate workaround\", \"id\": \"j-e90f0fd51c3b473cb79184273ed224c3\", \"status\": \"finished\", \"title\": \"get_timestamps\", \"updated\": \"2022-08-08T07:15:35Z\"}, {\"created\": \"2022-08-09T10:32:29Z\", \"id\": \"j-daeae9152ef941389c66729f0c93ad02\", \"status\": \"finished\", \"title\": \"get_collection\", \"updated\": \"2022-08-09T10:33:02Z\"}, {\"created\": \"2022-08-05T14:22:10Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-a07f673123974f6e8cc2c1a7263e4cf5\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-05T14:22:39Z\"}, {\"created\": \"2022-08-05T12:47:39Z\", \"description\": \"get filtered datacube\", \"id\": \"j-41641109c6d14ad2bc4b6ea9bcb00654\", \"status\": \"finished\", \"title\": \"get_filtered_data\", \"updated\": \"2022-08-05T12:48:07Z\"}, {\"created\": \"2022-08-09T11:37:33Z\", \"id\": \"j-2e1e0cb753064cff84e9ba4de8a6fe9d\", \"status\": \"finished\", \"title\": \"t_bucketing_aquamonitor\", \"updated\": \"2022-08-09T11:38:04Z\"}, {\"created\": \"2022-08-09T13:53:40Z\", \"id\": \"j-52e23d709a5c40a6a8461b079b59c09e\", \"status\": \"finished\", \"title\": \"mndwi_calc\", \"updated\": \"2022-08-09T13:54:07Z\"}, {\"created\": \"2022-08-08T05:50:21Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-bce93fcc8ced4790b4a911e5063da655\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-08T05:50:54Z\"}, {\"created\": \"2022-08-03T15:00:52Z\", \"description\": \"download mndwi merge cube\", \"id\": \"j-1d85ae7ed2714f2986288c84c98c9df9\", \"status\": \"finished\", \"title\": \"mndwi_merging\", \"updated\": \"2022-08-03T15:01:22Z\"}, {\"created\": \"2022-08-04T10:53:56Z\", \"description\": \"get timestamps for aggregate workaround\", \"id\": \"j-06cae686f08649df868ddd71146f943b\", \"status\": \"finished\", \"title\": \"get_timestamps\", \"updated\": \"2022-08-04T10:54:22Z\"}, {\"created\": \"2022-08-08T07:56:18Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-1d198f7edf734aa8ad682eee6a83cfa5\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-08T07:56:45Z\"}, {\"created\": \"2022-08-09T10:17:42Z\", \"id\": \"j-fdb2ccd0573f4055929a7b7998d105c8\", \"status\": \"finished\", \"title\": \"get_collection\", \"updated\": \"2022-08-09T10:18:07Z\"}, {\"created\": \"2022-08-03T10:03:33Z\", \"description\": \"get filtered datacube\", \"id\": \"j-f2e1fa421c1c4513814108cfa403f131\", \"status\": \"finished\", \"title\": \"get_filtered_data\", \"updated\": \"2022-08-03T10:03:58Z\"}, {\"created\": \"2022-08-09T09:34:54Z\", \"id\": \"j-2e3f32c029294252b987940cfe281f29\", \"status\": \"finished\", \"title\": \"get_collection\", \"updated\": \"2022-08-09T10:10:28Z\"}, {\"created\": \"2022-08-03T14:15:14Z\", \"description\": \"test_quality_score\", \"id\": \"j-321964f25405430385e9fff42e60fae1\", \"status\": \"finished\", \"title\": \"quality_score\", \"updated\": \"2022-08-03T14:15:44Z\"}, {\"created\": \"2022-08-02T12:28:07Z\", \"description\": \"download mndwi merge cube\", \"id\": \"j-1c6fa58cf310491a8fc8cce9474046f1\", \"status\": \"finished\", \"title\": \"mndwi_merging\", \"updated\": \"2022-08-02T12:28:38Z\"}, {\"created\": \"2022-08-02T11:14:48Z\", \"description\": \"download water occurrence merge cube\", \"id\": \"j-3efd86a1a20947f59eff35ae1a566838\", \"status\": \"finished\", \"title\": \"dl_merging_wo\", \"updated\": \"2022-08-02T11:15:19Z\"}, {\"created\": \"2022-08-03T14:45:17Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-6b1ca1cfba3d4e36a5a3d2748f6c3426\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-03T14:45:54Z\"}, {\"created\": \"2022-08-08T11:49:25Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-c6245148cf324b1f856b4ed12ad3dc28\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-08T11:49:58Z\"}, {\"created\": \"2022-08-02T10:56:24Z\", \"description\": \"get timestamps for aggregate workaround\", \"id\": \"j-f2190ef194994f3bb228f7b469cd5d54\", \"status\": \"finished\", \"title\": \"get_timestamps\", \"updated\": \"2022-08-02T10:56:56Z\"}, {\"created\": \"2022-08-03T12:44:12Z\", \"description\": \"get filtered datacube\", \"id\": \"j-2123f903badb407fab58e6477bc62686\", \"status\": \"finished\", \"title\": \"get_filtered_data\", \"updated\": \"2022-08-03T12:44:42Z\"}, {\"created\": \"2022-08-03T10:15:57Z\", \"description\": \"get filtered datacube\", \"id\": \"j-7f5b01fc9a394b0e84b8d7668a26427a\", \"status\": \"finished\", \"title\": \"get_filtered_data\", \"updated\": \"2022-08-03T10:16:26Z\"}, {\"created\": \"2022-08-03T13:56:04Z\", \"description\": \"test_quality_score\", \"id\": \"j-d14c84d849cc44ca9f2a85bb25b0d91b\", \"status\": \"finished\", \"title\": \"quality_score\", \"updated\": \"2022-08-03T13:56:29Z\"}, {\"created\": \"2022-08-09T11:59:49Z\", \"id\": \"j-91179bc8a89245069444563e8141828d\", \"status\": \"finished\", \"title\": \"counting_buckets_aquamonitor\", \"updated\": \"2022-08-09T12:00:16Z\"}, {\"created\": \"2022-08-09T13:39:49Z\", \"id\": \"j-c179cd05b932494cb70c0fe55b800903\", \"status\": \"finished\", \"title\": \"mndwi_calculation\", \"updated\": \"2022-08-09T13:40:19Z\"}, {\"created\": \"2022-08-05T14:41:57Z\", \"description\": \"download water occurrence merge cube\", \"id\": \"j-7975a008eaf74b4aa9a7dc0c2eab160a\", \"status\": \"finished\", \"title\": \"dl_merging_wo\", \"updated\": \"2022-08-05T14:42:26Z\"}, {\"created\": \"2022-08-10T07:41:41Z\", \"id\": \"j-7b87f3a562f647739e6ab16eab6b117b\", \"status\": \"finished\", \"title\": \"linear_regression aquamonitor\", \"updated\": \"2022-08-10T07:42:09Z\"}, {\"created\": \"2022-08-04T10:54:21Z\", \"description\": \"get timestamps for aggregate workaround\", \"id\": \"j-a7ba2088fbc14cf4a39d9d8861858c82\", \"status\": \"finished\", \"title\": \"get_timestamps\", \"updated\": \"2022-08-04T10:54:53Z\"}, {\"created\": \"2022-08-02T13:16:27Z\", \"description\": \"download water occurrence merge cube\", \"id\": \"j-8ede393137184c668f738f3b0faef7a6\", \"status\": \"finished\", \"title\": \"dl_merging_wo\", \"updated\": \"2022-08-02T13:16:58Z\"}, {\"created\": \"2022-08-09T15:19:04Z\", \"id\": \"j-759d33e2ff014530946bf680b6b0ae3d\", \"status\": \"finished\", \"title\": \"mndwi_calculation\", \"updated\": \"2022-08-09T15:19:33Z\"}, {\"created\": \"2022-08-05T13:57:03Z\", \"description\": \"test_quality_score\", \"id\": \"j-aa1d35202dfb48af88f2f2aafb3903e2\", \"status\": \"finished\", \"title\": \"quality_score\", \"updated\": \"2022-08-05T13:57:34Z\"}, {\"created\": \"2022-08-05T14:42:26Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-f56481cddbc14b16805602f6c2eddf69\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-05T14:42:59Z\"}, {\"created\": \"2022-08-05T15:20:38Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-59e94e08b72f496681a3f314dee2e56c\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-05T15:21:11Z\"}, {\"created\": \"2022-08-04T11:15:15Z\", \"description\": \"merging water occurrence.\", \"id\": \"j-c0f5d2d04c02425dba69e782ebf62aef\", \"status\": \"finished\", \"title\": \"merging_wo\", \"updated\": \"2022-08-04T11:15:45Z\"}, {\"created\": \"2022-08-10T14:12:19Z\", \"description\": \"gww_udf\", \"id\": \"j-d3b2077aa25f4d78aaeb81cd2706bbdc\", \"status\": \"created\", \"title\": \"gww_udf\"}, {\"created\": \"2022-08-10T14:12:30Z\", \"description\": \"gww_udf\", \"id\": \"j-6f906607efed4d6cb7a369a6e07d42a0\", \"status\": \"running\", \"title\": \"gww_udf\", \"updated\": \"2022-08-10T14:12:58Z\"}, {\"created\": \"2022-08-08T13:29:55Z\", \"id\": \"j-f7efd091f8994ee5a47dda6454643025\", \"status\": \"running\", \"title\": \"get_collection\", \"updated\": \"2022-08-09T10:10:47Z\"}, {\"created\": \"2022-08-09T08:27:03Z\", \"id\": \"j-450f4d0ab2bd4f24bf148c2b75e07f54\", \"status\": \"running\", \"title\": \"get_collection\", \"updated\": \"2022-08-09T10:10:38Z\"}, {\"created\": \"2022-08-03T13:55:15Z\", \"description\": \"test_quality_score\", \"id\": \"j-d1f2ec9dd5ae457590f17d6119217786\", \"status\": \"created\", \"title\": \"quality_score\"}, {\"created\": \"2022-08-08T13:29:54Z\", \"id\": \"j-bdb70723736c435598e43022c6e67882\", \"status\": \"created\", \"title\": \"get_collection\"}]}</script>\n",
       "    </openeo-data-table>\n",
       "    "
      ],
      "text/plain": [
       "[{'created': '2022-08-02T12:46:33Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-0b80545f29df49a0b777ac196f3d2881',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-02T12:47:01Z'},\n",
       " {'created': '2022-08-08T07:15:09Z',\n",
       "  'description': 'get timestamps for aggregate workaround',\n",
       "  'id': 'j-e90f0fd51c3b473cb79184273ed224c3',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_timestamps',\n",
       "  'updated': '2022-08-08T07:15:35Z'},\n",
       " {'created': '2022-08-09T10:32:29Z',\n",
       "  'id': 'j-daeae9152ef941389c66729f0c93ad02',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_collection',\n",
       "  'updated': '2022-08-09T10:33:02Z'},\n",
       " {'created': '2022-08-05T14:22:10Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-a07f673123974f6e8cc2c1a7263e4cf5',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-05T14:22:39Z'},\n",
       " {'created': '2022-08-05T12:47:39Z',\n",
       "  'description': 'get filtered datacube',\n",
       "  'id': 'j-41641109c6d14ad2bc4b6ea9bcb00654',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_filtered_data',\n",
       "  'updated': '2022-08-05T12:48:07Z'},\n",
       " {'created': '2022-08-09T11:37:33Z',\n",
       "  'id': 'j-2e1e0cb753064cff84e9ba4de8a6fe9d',\n",
       "  'status': 'finished',\n",
       "  'title': 't_bucketing_aquamonitor',\n",
       "  'updated': '2022-08-09T11:38:04Z'},\n",
       " {'created': '2022-08-09T13:53:40Z',\n",
       "  'id': 'j-52e23d709a5c40a6a8461b079b59c09e',\n",
       "  'status': 'finished',\n",
       "  'title': 'mndwi_calc',\n",
       "  'updated': '2022-08-09T13:54:07Z'},\n",
       " {'created': '2022-08-08T05:50:21Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-bce93fcc8ced4790b4a911e5063da655',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-08T05:50:54Z'},\n",
       " {'created': '2022-08-03T15:00:52Z',\n",
       "  'description': 'download mndwi merge cube',\n",
       "  'id': 'j-1d85ae7ed2714f2986288c84c98c9df9',\n",
       "  'status': 'finished',\n",
       "  'title': 'mndwi_merging',\n",
       "  'updated': '2022-08-03T15:01:22Z'},\n",
       " {'created': '2022-08-04T10:53:56Z',\n",
       "  'description': 'get timestamps for aggregate workaround',\n",
       "  'id': 'j-06cae686f08649df868ddd71146f943b',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_timestamps',\n",
       "  'updated': '2022-08-04T10:54:22Z'},\n",
       " {'created': '2022-08-08T07:56:18Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-1d198f7edf734aa8ad682eee6a83cfa5',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-08T07:56:45Z'},\n",
       " {'created': '2022-08-09T10:17:42Z',\n",
       "  'id': 'j-fdb2ccd0573f4055929a7b7998d105c8',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_collection',\n",
       "  'updated': '2022-08-09T10:18:07Z'},\n",
       " {'created': '2022-08-03T10:03:33Z',\n",
       "  'description': 'get filtered datacube',\n",
       "  'id': 'j-f2e1fa421c1c4513814108cfa403f131',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_filtered_data',\n",
       "  'updated': '2022-08-03T10:03:58Z'},\n",
       " {'created': '2022-08-09T09:34:54Z',\n",
       "  'id': 'j-2e3f32c029294252b987940cfe281f29',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_collection',\n",
       "  'updated': '2022-08-09T10:10:28Z'},\n",
       " {'created': '2022-08-03T14:15:14Z',\n",
       "  'description': 'test_quality_score',\n",
       "  'id': 'j-321964f25405430385e9fff42e60fae1',\n",
       "  'status': 'finished',\n",
       "  'title': 'quality_score',\n",
       "  'updated': '2022-08-03T14:15:44Z'},\n",
       " {'created': '2022-08-02T12:28:07Z',\n",
       "  'description': 'download mndwi merge cube',\n",
       "  'id': 'j-1c6fa58cf310491a8fc8cce9474046f1',\n",
       "  'status': 'finished',\n",
       "  'title': 'mndwi_merging',\n",
       "  'updated': '2022-08-02T12:28:38Z'},\n",
       " {'created': '2022-08-02T11:14:48Z',\n",
       "  'description': 'download water occurrence merge cube',\n",
       "  'id': 'j-3efd86a1a20947f59eff35ae1a566838',\n",
       "  'status': 'finished',\n",
       "  'title': 'dl_merging_wo',\n",
       "  'updated': '2022-08-02T11:15:19Z'},\n",
       " {'created': '2022-08-03T14:45:17Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-6b1ca1cfba3d4e36a5a3d2748f6c3426',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-03T14:45:54Z'},\n",
       " {'created': '2022-08-08T11:49:25Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-c6245148cf324b1f856b4ed12ad3dc28',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-08T11:49:58Z'},\n",
       " {'created': '2022-08-02T10:56:24Z',\n",
       "  'description': 'get timestamps for aggregate workaround',\n",
       "  'id': 'j-f2190ef194994f3bb228f7b469cd5d54',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_timestamps',\n",
       "  'updated': '2022-08-02T10:56:56Z'},\n",
       " {'created': '2022-08-03T12:44:12Z',\n",
       "  'description': 'get filtered datacube',\n",
       "  'id': 'j-2123f903badb407fab58e6477bc62686',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_filtered_data',\n",
       "  'updated': '2022-08-03T12:44:42Z'},\n",
       " {'created': '2022-08-03T10:15:57Z',\n",
       "  'description': 'get filtered datacube',\n",
       "  'id': 'j-7f5b01fc9a394b0e84b8d7668a26427a',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_filtered_data',\n",
       "  'updated': '2022-08-03T10:16:26Z'},\n",
       " {'created': '2022-08-03T13:56:04Z',\n",
       "  'description': 'test_quality_score',\n",
       "  'id': 'j-d14c84d849cc44ca9f2a85bb25b0d91b',\n",
       "  'status': 'finished',\n",
       "  'title': 'quality_score',\n",
       "  'updated': '2022-08-03T13:56:29Z'},\n",
       " {'created': '2022-08-09T11:59:49Z',\n",
       "  'id': 'j-91179bc8a89245069444563e8141828d',\n",
       "  'status': 'finished',\n",
       "  'title': 'counting_buckets_aquamonitor',\n",
       "  'updated': '2022-08-09T12:00:16Z'},\n",
       " {'created': '2022-08-09T13:39:49Z',\n",
       "  'id': 'j-c179cd05b932494cb70c0fe55b800903',\n",
       "  'status': 'finished',\n",
       "  'title': 'mndwi_calculation',\n",
       "  'updated': '2022-08-09T13:40:19Z'},\n",
       " {'created': '2022-08-05T14:41:57Z',\n",
       "  'description': 'download water occurrence merge cube',\n",
       "  'id': 'j-7975a008eaf74b4aa9a7dc0c2eab160a',\n",
       "  'status': 'finished',\n",
       "  'title': 'dl_merging_wo',\n",
       "  'updated': '2022-08-05T14:42:26Z'},\n",
       " {'created': '2022-08-10T07:41:41Z',\n",
       "  'id': 'j-7b87f3a562f647739e6ab16eab6b117b',\n",
       "  'status': 'finished',\n",
       "  'title': 'linear_regression aquamonitor',\n",
       "  'updated': '2022-08-10T07:42:09Z'},\n",
       " {'created': '2022-08-04T10:54:21Z',\n",
       "  'description': 'get timestamps for aggregate workaround',\n",
       "  'id': 'j-a7ba2088fbc14cf4a39d9d8861858c82',\n",
       "  'status': 'finished',\n",
       "  'title': 'get_timestamps',\n",
       "  'updated': '2022-08-04T10:54:53Z'},\n",
       " {'created': '2022-08-02T13:16:27Z',\n",
       "  'description': 'download water occurrence merge cube',\n",
       "  'id': 'j-8ede393137184c668f738f3b0faef7a6',\n",
       "  'status': 'finished',\n",
       "  'title': 'dl_merging_wo',\n",
       "  'updated': '2022-08-02T13:16:58Z'},\n",
       " {'created': '2022-08-09T15:19:04Z',\n",
       "  'id': 'j-759d33e2ff014530946bf680b6b0ae3d',\n",
       "  'status': 'finished',\n",
       "  'title': 'mndwi_calculation',\n",
       "  'updated': '2022-08-09T15:19:33Z'},\n",
       " {'created': '2022-08-05T13:57:03Z',\n",
       "  'description': 'test_quality_score',\n",
       "  'id': 'j-aa1d35202dfb48af88f2f2aafb3903e2',\n",
       "  'status': 'finished',\n",
       "  'title': 'quality_score',\n",
       "  'updated': '2022-08-05T13:57:34Z'},\n",
       " {'created': '2022-08-05T14:42:26Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-f56481cddbc14b16805602f6c2eddf69',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-05T14:42:59Z'},\n",
       " {'created': '2022-08-05T15:20:38Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-59e94e08b72f496681a3f314dee2e56c',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-05T15:21:11Z'},\n",
       " {'created': '2022-08-04T11:15:15Z',\n",
       "  'description': 'merging water occurrence.',\n",
       "  'id': 'j-c0f5d2d04c02425dba69e782ebf62aef',\n",
       "  'status': 'finished',\n",
       "  'title': 'merging_wo',\n",
       "  'updated': '2022-08-04T11:15:45Z'},\n",
       " {'created': '2022-08-10T14:12:19Z',\n",
       "  'description': 'gww_udf',\n",
       "  'id': 'j-d3b2077aa25f4d78aaeb81cd2706bbdc',\n",
       "  'status': 'created',\n",
       "  'title': 'gww_udf'},\n",
       " {'created': '2022-08-10T14:12:30Z',\n",
       "  'description': 'gww_udf',\n",
       "  'id': 'j-6f906607efed4d6cb7a369a6e07d42a0',\n",
       "  'status': 'running',\n",
       "  'title': 'gww_udf',\n",
       "  'updated': '2022-08-10T14:12:58Z'},\n",
       " {'created': '2022-08-08T13:29:55Z',\n",
       "  'id': 'j-f7efd091f8994ee5a47dda6454643025',\n",
       "  'status': 'running',\n",
       "  'title': 'get_collection',\n",
       "  'updated': '2022-08-09T10:10:47Z'},\n",
       " {'created': '2022-08-09T08:27:03Z',\n",
       "  'id': 'j-450f4d0ab2bd4f24bf148c2b75e07f54',\n",
       "  'status': 'running',\n",
       "  'title': 'get_collection',\n",
       "  'updated': '2022-08-09T10:10:38Z'},\n",
       " {'created': '2022-08-03T13:55:15Z',\n",
       "  'description': 'test_quality_score',\n",
       "  'id': 'j-d1f2ec9dd5ae457590f17d6119217786',\n",
       "  'status': 'created',\n",
       "  'title': 'quality_score'},\n",
       " {'created': '2022-08-08T13:29:54Z',\n",
       "  'id': 'j-bdb70723736c435598e43022c6e67882',\n",
       "  'status': 'created',\n",
       "  'title': 'get_collection'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs = con.list_jobs()\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8144b53b-8956-454a-8e58-0f3f9cfbe97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job: BatchJob = BatchJob(\"j-f1704e4a51264e2a831e1e9c12f0a423\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a150d51d-fb95-42b9-925b-7ae73b5b3e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-logs')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-logs>\n",
       "        <script type=\"application/json\">{\"logs\": [{\"id\": \"error\", \"level\": \"error\", \"message\": \"error processing batch job\\nTraceback (most recent call last):\\n  File \\\"batch_job.py\\\", line 319, in main\\n    run_driver()\\n  File \\\"batch_job.py\\\", line 292, in run_driver\\n    run_job(\\n  File \\\"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeogeotrellis/utils.py\\\", line 43, in memory_logging_wrapper\\n    return function(*args, **kwargs)\\n  File \\\"batch_job.py\\\", line 388, in run_job\\n    assets_metadata = result.write_assets(str(output_file))\\n  File \\\"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeo_driver/save_result.py\\\", line 110, in write_assets\\n    return self.cube.write_assets(filename=directory, format=self.format, format_options=self.options)\\n  File \\\"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeogeotrellis/geopysparkdatacube.py\\\", line 1691, in write_assets\\n    asset_paths = self._get_jvm().org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters(\\n  File \\\"/opt/spark3_2_0/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\\\", line 1309, in __call__\\n    return_value = get_return_value(\\n  File \\\"/opt/spark3_2_0/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\\\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 142.0 failed 4 times, most recent failure: Lost task 2.3 in stage 142.0 (TID 1099) (epod118.vgt.vito.be executor 34): java.lang.IllegalArgumentException: The number of bands in the metadata 2/2 does not match the actual band count in the cubes (left/right): 2/1. You can fix this by explicitly specifying correct band labels.\\n\\tat org.openeo.geotrellis.OpenEOProcesses.$anonfun$combine_bands$1(OpenEOProcesses.scala:661)\\n\\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:751)\\n\\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\\n\\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4(CoGroupedRDD.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4$adapted(CoGroupedRDD.scala:154)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.partition.ReorderedSpaceRDD.compute(ReorderedRDD.scala:57)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\\n\\tat scala.Option.foreach(Option.scala:407)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\\n\\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.cacheAndRepartition(NetCDFRDDWriter.scala:211)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.saveSingleNetCDFGeneric(NetCDFRDDWriter.scala:123)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.saveSingleNetCDFGeneric(NetCDFRDDWriter.scala:105)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.writeRasters(NetCDFRDDWriter.scala:77)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters(NetCDFRDDWriter.scala)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\nCaused by: java.lang.IllegalArgumentException: The number of bands in the metadata 2/2 does not match the actual band count in the cubes (left/right): 2/1. You can fix this by explicitly specifying correct band labels.\\n\\tat org.openeo.geotrellis.OpenEOProcesses.$anonfun$combine_bands$1(OpenEOProcesses.scala:661)\\n\\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:751)\\n\\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\\n\\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4(CoGroupedRDD.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4$adapted(CoGroupedRDD.scala:154)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.partition.ReorderedSpaceRDD.compute(ReorderedRDD.scala:57)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\t... 1 more\\n\\n\"}]}</script>\n",
       "    </openeo-logs>\n",
       "    "
      ],
      "text/plain": [
       "[{'id': 'error',\n",
       "  'level': 'error',\n",
       "  'message': 'error processing batch job\\nTraceback (most recent call last):\\n  File \"batch_job.py\", line 319, in main\\n    run_driver()\\n  File \"batch_job.py\", line 292, in run_driver\\n    run_job(\\n  File \"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeogeotrellis/utils.py\", line 43, in memory_logging_wrapper\\n    return function(*args, **kwargs)\\n  File \"batch_job.py\", line 388, in run_job\\n    assets_metadata = result.write_assets(str(output_file))\\n  File \"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeo_driver/save_result.py\", line 110, in write_assets\\n    return self.cube.write_assets(filename=directory, format=self.format, format_options=self.options)\\n  File \"/data3/hadoop/yarn/local/usercache/openeo/appcache/application_1657503144471_46189/container_e5049_1657503144471_46189_01_000006/venv/lib/python3.8/site-packages/openeogeotrellis/geopysparkdatacube.py\", line 1691, in write_assets\\n    asset_paths = self._get_jvm().org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters(\\n  File \"/opt/spark3_2_0/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\", line 1309, in __call__\\n    return_value = get_return_value(\\n  File \"/opt/spark3_2_0/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\", line 326, in get_return_value\\n    raise Py4JJavaError(\\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters.\\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 142.0 failed 4 times, most recent failure: Lost task 2.3 in stage 142.0 (TID 1099) (epod118.vgt.vito.be executor 34): java.lang.IllegalArgumentException: The number of bands in the metadata 2/2 does not match the actual band count in the cubes (left/right): 2/1. You can fix this by explicitly specifying correct band labels.\\n\\tat org.openeo.geotrellis.OpenEOProcesses.$anonfun$combine_bands$1(OpenEOProcesses.scala:661)\\n\\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:751)\\n\\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\\n\\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4(CoGroupedRDD.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4$adapted(CoGroupedRDD.scala:154)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.partition.ReorderedSpaceRDD.compute(ReorderedRDD.scala:57)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\n\\nDriver stacktrace:\\n\\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\\n\\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\\n\\tat scala.Option.foreach(Option.scala:407)\\n\\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\\n\\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\\n\\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\\n\\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\\n\\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\\n\\tat org.apache.spark.rdd.RDD.count(RDD.scala:1253)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.cacheAndRepartition(NetCDFRDDWriter.scala:211)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.saveSingleNetCDFGeneric(NetCDFRDDWriter.scala:123)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.saveSingleNetCDFGeneric(NetCDFRDDWriter.scala:105)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter$.writeRasters(NetCDFRDDWriter.scala:77)\\n\\tat org.openeo.geotrellis.netcdf.NetCDFRDDWriter.writeRasters(NetCDFRDDWriter.scala)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\\n\\tat java.base/java.lang.Thread.run(Thread.java:829)\\nCaused by: java.lang.IllegalArgumentException: The number of bands in the metadata 2/2 does not match the actual band count in the cubes (left/right): 2/1. You can fix this by explicitly specifying correct band labels.\\n\\tat org.openeo.geotrellis.OpenEOProcesses.$anonfun$combine_bands$1(OpenEOProcesses.scala:661)\\n\\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:751)\\n\\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\\n\\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)\\n\\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\\n\\tat org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4(CoGroupedRDD.scala:155)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$4$adapted(CoGroupedRDD.scala:154)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\\n\\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\\n\\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.partition.ReorderedSpaceRDD.compute(ReorderedRDD.scala:57)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.$anonfun$compute$2(CoGroupedRDD.scala:140)\\n\\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\\n\\tat scala.collection.immutable.List.foreach(List.scala:392)\\n\\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\\n\\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:136)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat geotrellis.spark.ContextRDD.compute(ContextRDD.scala:36)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\\n\\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\\n\\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\t... 1 more\\n\\n'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.logs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
